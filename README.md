# LLM Analysis

Large Language Models (LLMs) are rapidly advancing, but evaluating their performance on diverse prompts—and understanding the factors that influence these outcomes—remains challenging.

This project investigates two core questions using data from the Chatbot Arena (Chiang et al., 2024):

Which chatbot model would a human user select as the winner for a given prompt?

How “hard” would ChatGPT-3.5 rate that prompt?

We approach these questions by building machine learning models based on prompt topics and textual properties such as sentiment and subjectivity.




